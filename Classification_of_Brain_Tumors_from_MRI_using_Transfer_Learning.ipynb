{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akazemi24/Research-Project/blob/main/Classification_of_Brain_Tumors_from_MRI_using_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xyr7YPIBCnwa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Armita Project/data"
      ],
      "metadata": {
        "id": "CxjXV4IdC0iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import cv2\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "HERpZjwGDroM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Importing images and labels\n",
        "'''\n",
        "\n",
        "img_size=224\n",
        "\n",
        "train_folders = os.listdir('/content/drive/MyDrive/Armita Project/data/Training/')\n",
        "train_folders = [folder for folder in train_folders if '.' not in folder]\n",
        "\n",
        "num_train_images = 0\n",
        "train_images = []\n",
        "train_labels = []\n",
        "\n",
        "base_path = '/content/drive/MyDrive/Armita Project/data/Training/'\n",
        "for folder in train_folders:\n",
        "  image_array = os.listdir(base_path + folder)\n",
        "  num_train_images += len(image_array)\n",
        "  for image in image_array:\n",
        "    img = np.asarray(mpimg.imread(os.path.join(base_path, folder, image)))\n",
        "    img = cv2.resize(img, (img_size, img_size)) # This size is used for everything else...but try changing it.\n",
        "    #img = cv2.resize(img, (224, 224)) # We need size 224 for our transfer training!\n",
        "    train_images.append((img/255.0).astype(np.float32))\n",
        "    if folder == 'no_tumor':\n",
        "      train_labels.append(0)\n",
        "    if folder == 'pituitary_tumor':\n",
        "      train_labels.append(1)\n",
        "    if folder == 'meningioma_tumor':\n",
        "      train_labels.append(2)\n",
        "    if folder == 'glioma_tumor':\n",
        "      train_labels.append(3)\n",
        "\n",
        "test_folder = os.listdir('/content/drive/MyDrive/Armita Project/data/Testing/')\n",
        "test_folder = [folder for folder in test_folder if '.' not in folder]\n",
        "\n",
        "num_test_images = 0\n",
        "test_images = []\n",
        "test_labels = []\n",
        "base_path = '/content/drive/MyDrive/Armita Project/data/Testing/'\n",
        "for folder in test_folder:\n",
        "  image_array = os.listdir(base_path + folder)\n",
        "  num_test_images += len(image_array)\n",
        "  for image in image_array:\n",
        "    img = np.asarray(mpimg.imread(os.path.join(base_path, folder, image)))\n",
        "    img = cv2.resize(img, (img_size, img_size))\n",
        "    #img = cv2.resize(img, (224, 224))\n",
        "    train_images.append((img/255.0).astype(np.float32))\n",
        "    if folder == 'no_tumor':\n",
        "      train_labels.append(0)\n",
        "    if folder == 'pituitary_tumor':\n",
        "      train_labels.append(1)\n",
        "    if folder == 'meningioma_tumor':\n",
        "      train_labels.append(2)\n",
        "    if folder == 'glioma_tumor':\n",
        "      train_labels.append(3)\n",
        "\n",
        "\n",
        "print(\"There are\", num_train_images, \"in our training set!\")"
      ],
      "metadata": {
        "id": "zgNBDtGFDvI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_images = num_train_images + num_test_images\n",
        "X = np.array(train_images)\n",
        "y = np.array(train_labels)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "McJg65wxDzcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Split the data into training and testing set\n",
        "'''\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(X, y, test_size = 0.2)\n"
      ],
      "metadata": {
        "id": "YN3RHKoXD3Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "imports for transfer learning\n",
        "'''\n",
        "\n",
        "from tqdm import trange\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import datasets\n",
        "#from sklearn.metrics import plot_confusion_matrix, ConfusionMatrixDisplay, confusion_matrix\n",
        "import tensorflow as tf\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "import numbers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "from tqdm import tqdm\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import seaborn as sns\n",
        "import time"
      ],
      "metadata": {
        "id": "Kl0VQEKJD_Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "transform data to be used with Pytorch\n",
        "'''\n",
        "\n",
        "data_transform=transforms.Compose([\n",
        "                      transforms.ToPILImage(),\n",
        "                      #transforms.GaussianBlur(5),\n",
        "                      #transforms.RandomHorizontalFlip(),\n",
        "                      #transforms.RandomVerticalFlip(),\n",
        "                      #transforms.RandomRotation(15),\n",
        "                      #transforms.RandomRotation((1,90)),\n",
        "                      transforms.ToTensor(),\n",
        "                      #transforms.Normalize([0.485], [0.229])\n",
        "                  ])"
      ],
      "metadata": {
        "id": "LdyjgUV8jtK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "prepare data to use with Pytorch\n",
        "'''\n",
        "\n",
        "# Convert the images and labels to PyTorch tensors\n",
        "train_images_torch = torch.stack([data_transform(image[:,:,0]) for image in train_images])\n",
        "train_labels_torch = torch.Tensor(train_labels)\n",
        "\n",
        "test_images_torch = torch.stack([transforms.ToTensor()(image[:,:,0]) for image in test_images])\n",
        "test_labels_torch = torch.Tensor(test_labels)\n",
        "\n",
        "# Create a TensorDataset, which allows access to the images and labels as tensors\n",
        "train_dataset = TensorDataset(train_images_torch, train_labels_torch)\n",
        "test_dataset = TensorDataset(test_images_torch, test_labels_torch)\n",
        "\n",
        "# Create a data loader, which provides an iterable over the dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "-DSKc2zdD_p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Transform data for transfer learning\n",
        "'''\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        # transforms.ToPILImage(),\n",
        "        # transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        #transforms.GaussianBlur(5),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        # transforms.ToPILImage(),\n",
        "        # transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Convert the images and labels to PyTorch tensors\n",
        "train_images_transfer = torch.stack([data_transforms[\"train\"](image) for image in train_images])\n",
        "train_labels_transfer = torch.Tensor(train_labels)\n",
        "\n",
        "test_images_transfer = torch.stack([data_transforms['val'](image) for image in test_images])\n",
        "test_labels_transfer = torch.Tensor(test_labels)\n",
        "\n",
        "# Create a TensorDataset, which allows access to the images and labels as tensors\n",
        "train_dataset_transfer = TensorDataset(train_images_transfer, train_labels_transfer)\n",
        "test_dataset_transfer = TensorDataset(test_images_transfer, test_labels_transfer)\n",
        "\n",
        "# Create a data loader, which provides an iterable over the dataset\n",
        "train_loader_transfer = DataLoader(train_dataset_transfer, batch_size=32, shuffle=True)\n",
        "test_loader_transfer = DataLoader(test_dataset_transfer, batch_size=32)"
      ],
      "metadata": {
        "id": "bWvOBjGuEG7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "importing transfer learning models from Pytorch\n",
        "'''\n",
        "\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "d6UPNMnEAGfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "'''\n",
        "Change parameter to freeze or train convolutional base\n",
        "'''\n",
        "\n",
        "for param in resnet18.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "in_features = resnet18.fc.in_features\n",
        "\n",
        "resnet18.fc = nn.Linear(in_features, 4)\n",
        "\n",
        "#print(resnet18)"
      ],
      "metadata": {
        "id": "c0PopmqBETmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50 = models.resnet50(pretrained=True)\n",
        "\n",
        "for param in resnet50.parameters():\n",
        "\tparam.requires_grad = False\n",
        "\n",
        "in_features = resnet50.fc.in_features\n",
        "\n",
        "resnet50.fc = nn.Linear(in_features, 4)\n",
        "\n",
        "modelOutputFeats = resnet50.fc.in_features\n",
        "\n",
        "#print(resnet50)"
      ],
      "metadata": {
        "id": "jpPm2UsMEVS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "for param in vgg16.parameters():\n",
        "\tparam.requires_grad = False\n",
        "\n",
        "\n",
        "vgg16.classifier[-1] = nn.Linear(in_features = 4096, out_features=4)\n",
        "\n",
        "#print(vgg16)"
      ],
      "metadata": {
        "id": "AChCjj4sOhNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "densenet = models.densenet161(pretrained=True)\n",
        "\n",
        "for param in densenet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "in_features = densenet.classifier.in_features\n",
        "\n",
        "densenet.classifier = nn.Linear(in_features, 4)\n",
        "\n",
        "#print(densenet)\n"
      ],
      "metadata": {
        "id": "n70y2DkVnhOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "googlenet = models.googlenet(pretrained=True)\n",
        "\n",
        "for param in googlenet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "in_features = googlenet.fc.in_features\n",
        "\n",
        "googlenet.fc = nn.Linear(in_features, 4)\n"
      ],
      "metadata": {
        "id": "oyFk8vUbnxai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "\n",
        "for param in shufflenet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "in_features = shufflenet.fc.in_features\n",
        "\n",
        "shufflenet.fc = nn.Linear(in_features, 4)\n",
        "\n",
        "#print(shufflenet)"
      ],
      "metadata": {
        "id": "uppvCkaupVuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "for param in mobilenet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "mobilenet.classifier[1] = nn.Linear(mobilenet.last_channel, 4)"
      ],
      "metadata": {
        "id": "c7NFqtFipqpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX4-1NXBaxTN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Define a training function to run model training\n",
        "'''\n",
        "def train(model, train_loader, test_loader, num_epochs):\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "  losses = [] # Store the losses in case you want to plot them after training!\n",
        "  val_losses = []\n",
        "  acc  = 0\n",
        "  accuracy = []\n",
        "  y_pred = []\n",
        "  best_model = None\n",
        "  best_accuracy = 0\n",
        "  pbar = trange(num_epochs, desc=\"loss: \", leave=True) # Gives you a nice progress bar to visualize the loss while training\n",
        "  for epoch in pbar:\n",
        "    epoch_losses = []\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.to(torch.device('cuda:0'))\n",
        "      labels = labels.type(torch.LongTensor).to(torch.device('cuda:0'))\n",
        "      optimizer.zero_grad()\n",
        "      class_predictions = model(inputs)\n",
        "      # print(class_predictions.shape)\n",
        "      preds = torch.argmax(class_predictions.T, 0)\n",
        "      y_pred = np.concatenate((y_pred, preds.cpu().detach().numpy()))\n",
        "      loss = loss_fn(class_predictions, labels)\n",
        "      # print(loss.item())\n",
        "      epoch_losses.append(loss.item())\n",
        "      pbar.set_description(f\"loss: {np.mean(losses):.4f} Acc: {acc:.4f}\")\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "    acc, val_loss, train_loss, predictions = test(model, test_loader, train_loader)\n",
        "    if acc > best_accuracy:\n",
        "      best_accuracy = acc\n",
        "\n",
        "    accuracy.append(acc)\n",
        "    val_losses.append(val_loss)\n",
        "    losses.append(train_loss)\n",
        "    # print(val_loss, train_loss)\n",
        "    pbar.set_description(f\"loss: {np.mean(losses):.4f} Acc: {acc:.4f}\")\n",
        "\n",
        "  print(\"Finished Training\")\n",
        "  return losses, val_losses, accuracy, y_pred, best_accuracy, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2gIvzUQa0Nf"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Define a testing function\n",
        "'''\n",
        "\n",
        "def test(model, test_loader, train_loader):\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  val_loss = []\n",
        "  predictions = np.array([])\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    startTime = time.time()\n",
        "    for data in test_loader:\n",
        "      images, labels = data\n",
        "      images = images.to(torch.device('cuda:0'))\n",
        "      labels = labels.type(torch.LongTensor).to(torch.device('cuda:0'))\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      predictions = np.concatenate((predictions, predicted.cpu().numpy()))\n",
        "\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      # print(outputs)\n",
        "      # print(labels)\n",
        "      # print(loss.item())\n",
        "      val_loss.append(loss.item())\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = []\n",
        "    for data in train_loader:\n",
        "      images, labels = data\n",
        "      images = images.to(torch.device('cuda:0'))\n",
        "      labels = labels.type(torch.LongTensor).to(torch.device('cuda:0'))\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      train_loss.append(loss.item())\n",
        "\n",
        "  model.train()\n",
        "  accuracy = correct / total\n",
        "  print(accuracy)\n",
        "  print(np.mean(val_loss))\n",
        "\n",
        "  endTime = time.time()\n",
        "  print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
        "    endTime - startTime))\n",
        "\n",
        "  return accuracy, np.mean(val_loss), np.mean(train_loss), predictions\n",
        "\n",
        "  # print(f'Accuracy of the network on the {len(test_loader)} test data: {100 * correct / total} %')\n",
        "\n",
        "  # display the total time needed to perform the training\n",
        "  endTime = time.time()\n",
        "  print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
        "    endTime - startTime))\n",
        "  # plot the training loss and accuracy\n",
        "\n",
        "  cm = confusion_matrix(y_test, outputs)\n",
        "  print(cm)\n",
        "\n",
        "  plt.figure(figsize=(4, 4))\n",
        "  sns.heatmap(cm, annot= True, fmt=\".3f\", linewidth = 0.5, square = True, cmap = \"Blues_r\")\n",
        "  plt.ylabel(\"Actual Label\")\n",
        "  plt.xlabel(\"Predicted Label\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Define function to create confusion matrix\n",
        "'''\n",
        "\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "def cm_analysis(y_true, y_pred, labels, classes, ymap=None, figsize=(17,17)):\n",
        "    sns.set(font_scale=1)\n",
        "\n",
        "    if ymap is not None:\n",
        "        y_pred = [ymap[yi] for yi in y_pred]\n",
        "        y_true = [ymap[yi] for yi in y_true]\n",
        "        labels = [ymap[yi] for yi in labels]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.2f%%\\n%d/%d' % (p, c, s)\n",
        "            #elif c == 0:\n",
        "            #    annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.2f%%\\n%d' % (p, c)\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels, normalize='true')\n",
        "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
        "    cm = cm * 100\n",
        "    cm.index.name = 'True Label'\n",
        "    cm.columns.name = 'Predicted Label'\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    plt.yticks(va='center')\n",
        "\n",
        "    sns.heatmap(cm, annot=annot, fmt='', ax=ax, xticklabels=classes, cbar=True, cbar_kws={'format':PercentFormatter()}, yticklabels=classes, cmap=\"Blues\")\n",
        "    plt.show"
      ],
      "metadata": {
        "id": "qA3jV6Qde6Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Define function to output metrics (confusion matrix, presision, recall, f1 score)\n",
        "'''\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import log_loss\n",
        "def metrics(actual, predicted):\n",
        "  cm_analysis(actual, predicted, labels=[0, 1, 2, 3], classes = [0, 1, 2, 3], figsize=(8, 8))\n",
        "  print ('Accuracy Score is',accuracy_score(actual, predicted))\n",
        "  print ('Classification Report : ')\n",
        "  print (classification_report(actual, predicted))\n",
        "  #print('AUC-ROC:',roc_auc_score(actual, predicted))\n",
        "  #print('LOGLOSS Value is',log_loss(actual, predicted))\n"
      ],
      "metadata": {
        "id": "WL1cjX27efA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgsymApNbKJS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Run transfer learning models\n",
        "'''\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "googlenet.to(device)\n",
        "\n",
        "losses, val_losses, accuracy, y_pred, best_accuracy, predictions = train(googlenet, train_loader_transfer, test_loader_transfer, 100)\n",
        "\n",
        "print(f\"best accuracy: {best_accuracy}\")\n",
        "cm = confusion_matrix(test_labels, predictions)\n",
        "print(cm)\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "sns.heatmap(cm, annot= True, fmt=\".3f\", linewidth = 0.5, square = True, cmap = \"Blues_r\")\n",
        "plt.ylabel(\"Actual Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "\n",
        "figure, axis = plt.subplots(1, 2)\n",
        "\n",
        "axis[0].plot(losses, linewidth=2.75)\n",
        "axis[0].plot(val_losses, linewidth=2.75)\n",
        "axis[0].set_xlabel('epoch')\n",
        "axis[0].set_ylabel('Loss')\n",
        "axis[0].legend(['Train', 'Test'])\n",
        "axis[0].set_title(\"Loss\")\n",
        "#axis[0].set_ylim([0, 1])\n",
        "\n",
        "axis[1].plot(accuracy, linewidth=2.75)\n",
        "axis[1].set_xlabel('epoch')\n",
        "axis[1].set_ylabel('Accuracy')\n",
        "axis[1].legend(['Test'])\n",
        "axis[1].set_title(\"Accuracy\")\n",
        "\n",
        "figure.set_size_inches(10, 6, forward=True)\n",
        "figure.tight_layout(pad = 1)\n",
        "\n",
        "metrics(test_labels, predictions)"
      ]
    }
  ]
}